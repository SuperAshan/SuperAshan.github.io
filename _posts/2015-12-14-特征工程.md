---
title: 数据挖掘系列之特征工程
date: 2015-12-14
layout: post
permalink: /blog/2015/12/14/特征工程.html
categories:
  - 数据挖掘
tags:
  - 特征工程，数据清洗，特征降维，增益，pca，互信息，归一化，离散化，数据采样，相关系数，卡方检验
---
*“几乎所有的数据挖掘项目80%的工作量在于数据准备，数据准备的80%的工作量在于数据清洗”，已经不记得的这句话到底出自哪里，但是作为数据挖掘的做基础的工作，往往能对结果形成至关重要的原因。*
    
    1.数据准备（又名特征工程）：数据准备一般指从开始抓取数据到将数据转化为合适的形式进入我们所做的模型这个阶段的工作

要想做好数据挖掘的工作，必须在数据准备阶段给予足够的重视。此次博客将会详细的介绍数据准备的各个方面，但是由于博主经验有限不可能将方方面卖弄全都写到，只能抛砖引玉，希望看到博文的各个前辈高人提出宝贵的意见。

我们结合一个特征工程的整体框架图来看，更加容易理解

！[特征工程整体框架] （http://ashan.info/images/20151214150719_chactor.jpg）

##特征使用方案
首先获取特征，主要对于因变量有影响的特征都应该列入此范畴。所谓因变量就是结果，如分类结果，预测结果等。以文本分类为例，那词语，文本的长度，或者文本之中含有的特别的信心均可作为文本的特征。博主做过网站的分类，主要用到了网站的首页信息。则首页的中图片的个数，链接的个数，文本长度等等均可作为特征。

其次是可用性评估，可用性评估主要是考虑几个方面，首先获得某个特征的难度，如对于网站而言，获得网站的ip，title，keyword等可以通过直接的网页分析就可获得，但是对于网站拥有的页面数，以及每个页面的相关特征就需要一定的成本。然后是特征的覆盖率。特征的覆盖率计算一般有两个作用，一个是特征选择，通过覆盖率看这个特征是否可以模型的数据，还有就是通过特征的覆盖率监控发现故障，如某个特征的覆盖率出现异常波动，那肯定是数据源到特征流程出现问题。最后是对数据的准确率的特征的评估。由于每个特征是通过程序自动生成，而且往往是通过少量的训练集的计算得出，则肯定不能考虑所有的情况。如网页分类中，提取网页的关键字等，大多数网页会遵循一定的规则，通过html分析得到，但是还是会有部分网站网页结构并不一样，这样在我们提取的时候的出现错误，直接影响了特征的生成。

##特征获取方案
知道了要用哪些特征，如何得到这些特征是一个问题。一般得到特征对于网页，就是爬虫爬取，页面解析等，如资源较大，可采用分布式爬虫，如nutch，scrapy等框架，也可以自己写爬虫框架，根据需求来。至于如何存储，简单一点就是文件就可，当然做成mysql，mongodb或者直接在Hadoop上计算，博主一般通过Hadoop计算文件交互就可满足大部分需求

##特征处理

特征处理包括特征清洗，特征预处理。特征清洗主要是又分为异常样本的处理以及数据采样。
异常值处理主要的问题在于如何发现“异常点”，这里的异常点是在数据集中与众不同的数据，使人怀疑这些数据并非随机偏差， 而是产生于完全不同的机制。。常见的异常点检测算法包括：

- 偏差检测，如聚类（异常点不在任意一个类中）、序列异常、最近邻居法、多维数据分析等
- 基于统计的异常点检测。在一直数据的一定分布的情况下，通过数据的变异指标对数据检测。这些异常指标包含了极差，四分位数间距，均差，标准差，变异系数等，变异指标较大表明数据分布较为分散，反之，数据比较密集。但是也存在问题就是通常我们拿到的数据并不能确定他的分布，即使可以通过数据拟合得到数据的分布，也只能在一定程度上代表整个分布，且需要付出一定的计算代价
- 基于距离的异常点检测;可以计算与其他点距离的平均值最大的N个数据。或者计算与第k个最近邻居距离最大的n个对象，或者与k个最近邻居的平均距离最大的n个对象。还有一些比较成熟的算法如基于索引的算法。通常情况下是构造k-d树(在最近邻中用到），设定M为异常点数据的d领域的最大对象数目，如何对象o的M+1个邻居被发现，则o为正常点。复杂度为O(k*n^2),k为位数，n为数据集的数目。
- 基于密度的异常点检测。基于密度的异常点检测可以检测出局部异常，这是距离异常点锁不能发现的。最著名的是LOF算法。



###去重方案
bloomfilter，指纹算法去重，已经实现simhash